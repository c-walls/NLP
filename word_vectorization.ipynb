{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import compress\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 202651\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "    words = [word for line in f.readlines() for word in line.split()]\n",
    "\n",
    "print(f'Number of words: {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, deque\n",
    "from typing import List, Tuple\n",
    "\n",
    "def prepareData(words: List[str], vocab_size: int = 50000):\n",
    "    \"\"\"\n",
    "    Prepares the data for word vectorization by converting words to indices and creating dictionaries for word-to-index and index-to-word mappings.\n",
    "\n",
    "    Parameters:\n",
    "    words:      The corpus of words to be processed.\n",
    "    vocab_size: The maximum size of the vocabulary. Default is 50,000.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[int], List[Tuple[str, int]], dict, dict]:\n",
    "        - data:        The corpus converted to a list of word indices.\n",
    "        - count:       A list of tuples where each tuple contains a word and its frequency, including the <unk> token for rare words.\n",
    "        - dictionary:  A dictionary mapping words to their corresponding indices.\n",
    "        - reverse_dictionary:  A dictionary mapping indices to their corresponding words.\n",
    "    \"\"\"\n",
    "    ## Rare words are replaced by <unk> token\n",
    "    count = [['<unk>', -1]]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    ## Initialize dictionary for index to word mapping\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    ## Convert corpus to list of indices\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram (data: List[int], batch_size: int, num_skips: int, skip_window: int, data_index: int = 0):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for the skip-gram model.\n",
    "\n",
    "    Parameters:\n",
    "    data:        List of word indices.\n",
    "    batch_size:  Number of words in each batch.\n",
    "    num_skips:   How many times to reuse an input to generate a label.\n",
    "    skip_window: How many words to consider left and right.\n",
    "    data_index:  Index to start with in the data list. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Batch of input words and corresponding labels.\n",
    "    \"\"\"\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    window_size = 2 * skip_window + 1\n",
    "\n",
    "    # Create a buffer to store the data\n",
    "    buffer = deque(maxlen=window_size)\n",
    "    for _ in range(window_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Generates the batch of context words and labels\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, window_size - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "        # Move the window\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow(data: List[int], batch_size: int, num_skips: int, skip_window: int, data_index: int = 0):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for the CBOW model.\n",
    "\n",
    "    Parameters:\n",
    "    data:        List of word indices.\n",
    "    batch_size:  Number of words in each batch.\n",
    "    num_skips:   How many times to reuse an input to generate a label.\n",
    "    skip_window: How many words to consider left and right.\n",
    "    data_index:  Index to start with in the data list. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Batch of context words and corresponding labels.\n",
    "    \"\"\"    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size, num_skips), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    window_size = 2 * skip_window + 1\n",
    "    \n",
    "    # Create a buffer to store the data\n",
    "    buffer = deque(maxlen=window_size)\n",
    "    for _ in range(window_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Generates the batch of context words and labels\n",
    "    for i in range(batch_size):\n",
    "        mask = [1] * window_size\n",
    "        mask[skip_window] = 0\n",
    "        batch[i] = list(compress(buffer, mask))\n",
    "        labels[i, 0] = buffer[skip_window]\n",
    "\n",
    "        # Move the window\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words: ['First', 'Citizen:', 'we', 'proceed'], Target word: Before\n",
      "Context words: ['Citizen:', 'Before', 'proceed', 'any'], Target word: we\n",
      "Context words: ['Before', 'we', 'any', 'further,'], Target word: proceed\n",
      "Context words: ['we', 'proceed', 'further,', 'hear'], Target word: any\n",
      "Context words: ['proceed', 'any', 'hear', 'me'], Target word: further,\n"
     ]
    }
   ],
   "source": [
    "## CBOW is functioning - Run below to check  ----XXXX---- Note: punctation is not removed from the text\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = prepareData(words)\n",
    "\n",
    "# Call the cbow function\n",
    "batch_size = 8\n",
    "num_skips = 4\n",
    "skip_window = 2\n",
    "batch, labels = cbow(data, batch_size, num_skips, skip_window)\n",
    "\n",
    "# Print the first 5 examples\n",
    "for i in range(5):\n",
    "    context_words = [reverse_dictionary[idx] for idx in batch[i]]\n",
    "    target_word = reverse_dictionary[labels[i, 0]]\n",
    "    print(f\"Context words: {context_words}, Target word: {target_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
