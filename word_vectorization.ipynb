{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "from itertools import compress\n",
    "from collections import Counter, deque\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "- Fix Skipgram\n",
    "- Fix out of vocab_size index issues\n",
    "- Build test method/function for use when imported\n",
    "- Test word2vec embeddings with wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_file) as f:\n",
    "    words = [word for line in f.readlines() for word in line.split()]\n",
    "\n",
    "print(f'Number of words: {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|██████████| 19845159647/19845159647 [26:03<00:00, 12690909.99it/s]\n",
      "Converting words to indices: 100%|██████████| 19845159647/19845159647 [30:52<00:00, 10713579.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 500000\n",
      "Most common words: [['<unk>', 23882695], ('the', 197426772), ('of', 102327621), ('in', 86486718), ('and', 82223696), ('a', 56513157), ('to', 55008144), ('was', 33149692), ('is', 24429821), ('for', 24130351), ('on', 23603557), ('as', 23495232), ('by', 21107004), ('with', 19885853), ('from', 16931502), ('he', 16594540), ('at', 16368794), ('that', 14803061), ('his', 13090739), ('it', 11529692)]\n",
      "Sample data: [22110     8     5   248  2146     4   837    17     8 15268]\n",
      "Decoded sample data: ['anarchism', 'is', 'a', 'political', 'philosophy', 'and', 'movement', 'that', 'is', 'skeptical']\n"
     ]
    }
   ],
   "source": [
    "def preparePickleData(pickle_file_path: str, vocab_size: int = 50000) -> Tuple[np.ndarray, List[Tuple[str, int]], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Prepares the data for word vectorization by converting words to indices and creating dictionaries for word-to-index and index-to-word mappings.\n",
    "\n",
    "    Parameters:\n",
    "    pickle_file_path: The path to the pickle file containing the corpus of words to be processed.\n",
    "    vocab_size:       The maximum size of the vocabulary. Default is 50,000.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, List[Tuple[str, int]], dict, dict]:\n",
    "        - data:        The corpus converted to a NumPy array of word indices.\n",
    "        - count:       A list of tuples where each tuple contains a word and its frequency, including the <unk> token for rare words.\n",
    "        - dictionary:  A dictionary mapping words to their corresponding indices.\n",
    "        - reverse_dictionary:  A dictionary mapping indices to their corresponding words.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    # First pass: Count unique words\n",
    "    with open(pickle_file_path, \"rb\") as file:\n",
    "        file.seek(0, 2)  # Move to the end of the file to get its size\n",
    "        file_size = file.tell()\n",
    "        file.seek(0)  # Move back to the start of the file\n",
    "        with tqdm(total=file_size, desc=\"Counting words\") as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    words = pickle.load(file)\n",
    "                    words = [word.lower().translate(translator) for word in words]\n",
    "                    words = [word for word in words if word.isalpha()]\n",
    "                    word_counts.update(words)\n",
    "                    pbar.update(file.tell() - pbar.n)\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "    # Rare words are replaced by <unk> token\n",
    "    count = [['<unk>', -1]]\n",
    "    count.extend(word_counts.most_common(vocab_size - 1))\n",
    "\n",
    "    # Initialize dictionary for index to word mapping\n",
    "    dictionary = {word: idx for idx, (word, _) in enumerate(count)}\n",
    "    reverse_dictionary = {idx: word for word, idx in dictionary.items()}\n",
    "    \n",
    "    # Estimate the total number of words for the second pass\n",
    "    total_words = sum(word_counts.values())\n",
    "    \n",
    "    # Initialize NumPy array for word indices\n",
    "    data = np.zeros(total_words, dtype=np.int32)\n",
    "    \n",
    "    # Second pass: Convert words to indices\n",
    "    index = 0\n",
    "    unk_count = 0\n",
    "    with open(pickle_file_path, \"rb\") as file:\n",
    "        with tqdm(total=file_size, desc=\"Converting words to indices\") as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    words = pickle.load(file)\n",
    "                    words = [word.lower().translate(translator) for word in words]\n",
    "                    words = [word for word in words if word.isalpha()]\n",
    "                    for word in words:\n",
    "                        if word in dictionary:\n",
    "                            data[index] = dictionary[word]\n",
    "                        else:\n",
    "                            data[index] = 0\n",
    "                            unk_count += 1\n",
    "                        index += 1\n",
    "                    pbar.update(file.tell() - pbar.n)\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "# Example usage\n",
    "pickle_file_path = 'Resources\\\\corpus.pkl'\n",
    "data, count, dictionary, reverse_dictionary = preparePickleData(pickle_file_path, vocab_size=500000)\n",
    "print(f\"\\nTotal number of words: {len(data)}\")\n",
    "print(f\"Total number of unique words: {len(dictionary)}\")\n",
    "print(f\"Most common words: {count[:5]}\")\n",
    "print(f\"\\nSample data: {data[:10]}\")\n",
    "print(f\"Decoded sample data: {[reverse_dictionary[idx] for idx in data[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(words: List[str], vocab_size: int = 50000):\n",
    "    \"\"\"\n",
    "    Prepares the data for word vectorization by converting words to indices and creating dictionaries for word-to-index and index-to-word mappings.\n",
    "\n",
    "    Parameters:\n",
    "    words:      The corpus of words to be processed.\n",
    "    vocab_size: The maximum size of the vocabulary. Default is 50,000.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[int], List[Tuple[str, int]], dict, dict]:\n",
    "        - data:        The corpus converted to a list of word indices.\n",
    "        - count:       A list of tuples where each tuple contains a word and its frequency, including the <unk> token for rare words.\n",
    "        - dictionary:  A dictionary mapping words to their corresponding indices.\n",
    "        - reverse_dictionary:  A dictionary mapping indices to their corresponding words.\n",
    "    \"\"\"\n",
    "    ## Corpus pre-processing\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.lower().translate(translator) for word in words]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    ## Rare words are replaced by <unk> token\n",
    "    count = [['<unk>', -1]]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    ## Initialize dictionary for index to word mapping\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    ## Convert corpus to list of indices\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram (data: List[int], batch_size: int, num_skips: int, skip_window: int, data_index: int = 0):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for the skip-gram model.\n",
    "\n",
    "    Parameters:\n",
    "    data:        List of word indices.\n",
    "    batch_size:  Number of words in each batch.\n",
    "    num_skips:   How many times to reuse an input to generate a label.\n",
    "    skip_window: How many words to consider left and right.\n",
    "    data_index:  Index to start with in the data list. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Batch of input words and corresponding labels.\n",
    "    \"\"\"\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    window_size = 2 * skip_window + 1\n",
    "\n",
    "    # Create a buffer to store the data\n",
    "    buffer = deque(maxlen=window_size)\n",
    "    for _ in range(window_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Generates the batch of context words and labels\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, window_size - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "        # Move the window\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow(data: List[int], batch_size: int, num_skips: int, skip_window: int, data_index: int = 0):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for the CBOW model.\n",
    "\n",
    "    Parameters:\n",
    "    data:        List of word indices.\n",
    "    batch_size:  Number of words in each batch.\n",
    "    num_skips:   How many times to reuse an input to generate a label.\n",
    "    skip_window: How many words to consider left and right.\n",
    "    data_index:  Index to start with in the data list. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Batch of context words and corresponding labels.\n",
    "    \"\"\"    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size, num_skips), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    window_size = 2 * skip_window + 1\n",
    "    \n",
    "    # Create a buffer to store the data\n",
    "    buffer = deque(maxlen=window_size)\n",
    "    for _ in range(window_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Generates the batch of context words and labels\n",
    "    for i in range(batch_size):\n",
    "        mask = [1] * window_size\n",
    "        mask[skip_window] = 0\n",
    "        batch[i] = list(compress(buffer, mask))\n",
    "        labels[i, 0] = buffer[skip_window]\n",
    "\n",
    "        # Move the window\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CBOW is functioning / Skipgram is not functioning -- Code below can be made into a debug function\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = prepareData(words)\n",
    "\n",
    "# Call the cbow function\n",
    "batch_size = 8\n",
    "num_skips = 4\n",
    "skip_window = 2\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Corpus length: {len(data)}\")\n",
    "print(f\"Encoded corpus sample: {data[:20]}\")\n",
    "print(f\"Decoded corpus sample: {[reverse_dictionary[i] for i in data[:20]]}\")\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(count)}\")\n",
    "print(f\"Index Map size: {len(dictionary)}\")\n",
    "print(f\"\\nMost common words:\\n{sorted(count, key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "print(\"Index Map Examples:\\n\", {k: reverse_dictionary[k] for k in list(reverse_dictionary)[:5]})\n",
    "\n",
    "# Print the first 5 examples of CBOW batches\n",
    "print(\"\\nExamples of CBOW batches:\")\n",
    "cbow_batch, cbow_labels = cbow(data, batch_size, num_skips, skip_window)\n",
    "print(f\"Batch shape: {cbow_batch.shape}\")\n",
    "print(f\"Labels shape: {cbow_labels.shape}\")\n",
    "for i in range(5):\n",
    "    context_words = [reverse_dictionary[idx] for idx in cbow_batch[i]]\n",
    "    target_word = reverse_dictionary[cbow_labels[i, 0]]\n",
    "    print(f\"Context words: {context_words}, Target word: {target_word}\")\n",
    "\n",
    "# Print the first 5 examples of Skip-gram batches\n",
    "print(\"\\nExamples of Skip-gram batches:\")\n",
    "skipgram_batch, skipgram_labels = skipgram(data, batch_size, num_skips, skip_window)\n",
    "print(f\"Batch shape: {skipgram_batch.shape}\")\n",
    "print(f\"Labels shape: {skipgram_labels.shape}\")\n",
    "for i in range(5):\n",
    "    input_words = [reverse_dictionary[idx] for idx in skipgram_batch]\n",
    "    output_word = reverse_dictionary[skipgram_labels[i, 0]]\n",
    "    print(f\"Input word: {input_words}, Output word: {output_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocab_size: int = 50000,\n",
    "                 batch_size: int = 128,\n",
    "                 embedding_size: int = 128,\n",
    "                 architecture: str = 'skip-gram',\n",
    "                 num_skips: int = 2,\n",
    "                 skip_window: int = 1,\n",
    "                 loss_type: str = 'sampled_softmax_loss',\n",
    "                 n_neg_samples: int = 64,\n",
    "                 optimizer: str = 'adagrad',\n",
    "                 learning_rate: float = 1.0,\n",
    "                 n_steps: int = 10001,\n",
    "                 valid_size: int = 16,\n",
    "                 valid_window: int = 100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.architecture = architecture\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        self.loss_type = loss_type\n",
    "        self.n_neg_samples = n_neg_samples\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_steps = n_steps\n",
    "        self.valid_size = valid_size\n",
    "        self.valid_window = valid_window\n",
    "\n",
    "        self.chooseSamples()\n",
    "        self.chooseGenerator()\n",
    "        self.__init__model()\n",
    "\n",
    "    def chooseSamples(self):\n",
    "        valid_examples = np.array(random.sample(range(self.valid_window), self.valid_size))\n",
    "        self.valid_examples = valid_examples\n",
    "    \n",
    "    def chooseGenerator(self):\n",
    "        if self.architecture == 'skip-gram':\n",
    "            self.generator = skipgram\n",
    "        elif self.architecture == 'cbow':\n",
    "            self.generator = cbow\n",
    "        else:\n",
    "            raise ValueError(\"Architecture must be either 'skip-gram' or 'cbow'.\")\n",
    "        \n",
    "    def tokenMapping(self, words):\n",
    "        data, count, dictionary, reverse_dictionary = prepareData(words, self.vocab_size)\n",
    "        self.data = data\n",
    "        self.count = count\n",
    "        self.dictionary = dictionary\n",
    "        self.reverse_dictionary = reverse_dictionary\n",
    "        return data\n",
    "    \n",
    "    def __init__model(self):\n",
    "        self.embeddings = tf.Variable(tf.random.uniform([self.vocab_size, self.embedding_size], -1.0, 1.0))\n",
    "        self.weights = tf.Variable(tf.random.truncated_normal([self.vocab_size, self.embedding_size], stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "        self.biases = tf.Variable(tf.zeros([self.vocab_size]))\n",
    "\n",
    "        if self.optimizer == 'adagrad':\n",
    "            self.optimizer = tf.optimizers.Adagrad(learning_rate=self.learning_rate)\n",
    "        elif self.optimizer == 'SGD':\n",
    "            self.optimizer = tf.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compute the similarity distance metrics between individual embeddings\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(self.embeddings), 1, keepdims=True))\n",
    "        self.normalized_embeddings = self.embeddings / norm\n",
    "        self.valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)\n",
    "        self.valid_embeddings = tf.nn.embedding_lookup(self.normalized_embeddings, self.valid_dataset)\n",
    "        self.similarity = tf.matmul(self.valid_embeddings, self.normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, batch_data, batch_labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            if self.architecture == 'skip-gram':\n",
    "                embed = tf.nn.embedding_lookup(self.embeddings, batch_data)\n",
    "            elif self.architecture == 'cbow':\n",
    "                embed = tf.zeros([self.batch_size, self.embedding_size])\n",
    "                for j in range(self.num_skips):\n",
    "                    embed += tf.nn.embedding_lookup(self.embeddings, batch_data[:, j])\n",
    "                embed /= self.num_skips\n",
    "        \n",
    "            if self.loss_type == 'sampled_softmax_loss':\n",
    "                loss = tf.nn.sampled_softmax_loss(weights=self.weights,\n",
    "                                                  biases=self.biases,\n",
    "                                                  labels=batch_labels,\n",
    "                                                  inputs=embed,\n",
    "                                                  num_sampled=self.n_neg_samples,\n",
    "                                                  num_classes=self.vocab_size)\n",
    "            elif self.loss_type == 'nce_loss':\n",
    "                loss = tf.nn.nce_loss(weights=self.weights,\n",
    "                                      biases=self.biases,\n",
    "                                      labels=batch_labels,\n",
    "                                      inputs=embed,\n",
    "                                      num_sampled=self.n_neg_samples,\n",
    "                                      num_classes=self.vocab_size)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, [self.embeddings, self.weights, self.biases])\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.embeddings, self.weights, self.biases]))\n",
    "        return loss\n",
    "\n",
    "    def fit(self, words):\n",
    "        self.data = self.tokenMapping(words)\n",
    "        average_loss = 0\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            batch_data, batch_labels = self.generator(self.data, self.batch_size, self.num_skips, self.skip_window)\n",
    "            loss = self.train_step(batch_data, batch_labels)\n",
    "            average_loss += loss\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                print(f'Average loss at step {step}: {average_loss}')\n",
    "                average_loss = 0\n",
    "\n",
    "        self.final_embeddings = self.normalized_embeddings.numpy()\n",
    "        return self\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        if word not in self.dictionary:\n",
    "            raise ValueError(f\"Word '{word}' not in dictionary\")\n",
    "        word_index = self.dictionary[word]\n",
    "        return self.final_embeddings[word_index]\n",
    "\n",
    "    def similar_by_word(self, word, top_n=5):\n",
    "        word_vector = self.get_embedding(word)\n",
    "        similarities = np.dot(self.final_embeddings, word_vector) / (np.linalg.norm(self.final_embeddings, axis=1) * np.linalg.norm(word_vector))\n",
    "        similar_indices = np.argsort(-similarities)[:top_n]\n",
    "        \n",
    "        print(f\"Similar indices for '{word}': {similar_indices}\")\n",
    "        similar_words = []\n",
    "        for idx in similar_indices:\n",
    "            if idx in self.reverse_dictionary:\n",
    "                similar_words.append(self.reverse_dictionary[idx])\n",
    "            else:\n",
    "                print(f\"Index {idx} not found in reverse_dictionary\")\n",
    "                similar_words.append(f\"Index {idx} not found\")\n",
    "        \n",
    "        return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the cbow function\n",
    "batch_size = 128\n",
    "num_skips = 4\n",
    "skip_window = 2\n",
    "\n",
    "word2vec = Word2Vec(architecture='cbow', batch_size=batch_size, num_skips=num_skips, skip_window=skip_window)\n",
    "model = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the embedding for a word\n",
    "try:\n",
    "    embedding = model.get_embedding('king')\n",
    "    print(f\"Embedding for 'king': {embedding}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Find similar words\n",
    "try:\n",
    "    similar_words = model.similar_by_word('king')\n",
    "    print(f\"Words similar to 'king': {similar_words}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
