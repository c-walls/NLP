{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import compress\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 202651\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "    words = [word for line in f.readlines() for word in line.split()]\n",
    "\n",
    "print(f'Number of words: {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from collections import Counter, deque\n",
    "from typing import List, Tuple\n",
    "\n",
    "def prepareData(words: List[str], vocab_size: int = 50000):\n",
    "    \"\"\"\n",
    "    Prepares the data for word vectorization by converting words to indices and creating dictionaries for word-to-index and index-to-word mappings.\n",
    "\n",
    "    Parameters:\n",
    "    words:      The corpus of words to be processed.\n",
    "    vocab_size: The maximum size of the vocabulary. Default is 50,000.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[int], List[Tuple[str, int]], dict, dict]:\n",
    "        - data:        The corpus converted to a list of word indices.\n",
    "        - count:       A list of tuples where each tuple contains a word and its frequency, including the <unk> token for rare words.\n",
    "        - dictionary:  A dictionary mapping words to their corresponding indices.\n",
    "        - reverse_dictionary:  A dictionary mapping indices to their corresponding words.\n",
    "    \"\"\"\n",
    "    ## Corpus pre-processing\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.lower().translate(translator) for word in words]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    ## Rare words are replaced by <unk> token\n",
    "    count = [['<unk>', -1]]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    ## Initialize dictionary for index to word mapping\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    ## Convert corpus to list of indices\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram (data: List[int], batch_size: int, num_skips: int, skip_window: int, data_index: int = 0):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for the skip-gram model.\n",
    "\n",
    "    Parameters:\n",
    "    data:        List of word indices.\n",
    "    batch_size:  Number of words in each batch.\n",
    "    num_skips:   How many times to reuse an input to generate a label.\n",
    "    skip_window: How many words to consider left and right.\n",
    "    data_index:  Index to start with in the data list. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Batch of input words and corresponding labels.\n",
    "    \"\"\"\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    window_size = 2 * skip_window + 1\n",
    "\n",
    "    # Create a buffer to store the data\n",
    "    buffer = deque(maxlen=window_size)\n",
    "    for _ in range(window_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Generates the batch of context words and labels\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, window_size - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "        # Move the window\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow(data: List[int], batch_size: int, num_skips: int, skip_window: int, data_index: int = 0):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for the CBOW model.\n",
    "\n",
    "    Parameters:\n",
    "    data:        List of word indices.\n",
    "    batch_size:  Number of words in each batch.\n",
    "    num_skips:   How many times to reuse an input to generate a label.\n",
    "    skip_window: How many words to consider left and right.\n",
    "    data_index:  Index to start with in the data list. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Batch of context words and corresponding labels.\n",
    "    \"\"\"    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size, num_skips), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    window_size = 2 * skip_window + 1\n",
    "    \n",
    "    # Create a buffer to store the data\n",
    "    buffer = deque(maxlen=window_size)\n",
    "    for _ in range(window_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Generates the batch of context words and labels\n",
    "    for i in range(batch_size):\n",
    "        mask = [1] * window_size\n",
    "        mask[skip_window] = 0\n",
    "        batch[i] = list(compress(buffer, mask))\n",
    "        labels[i, 0] = buffer[skip_window]\n",
    "\n",
    "        # Move the window\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 202619\n",
      "Encoded corpus sample: [88, 267, 137, 35, 967, 142, 663, 124, 15, 104, 33, 104, 104, 88, 267, 6, 40, 33, 1256, 342]\n",
      "Decoded corpus sample: ['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather']\n",
      "\n",
      "Vocabulary size: 12848\n",
      "Index Map size: 12848\n",
      "\n",
      "Most common words:\n",
      "[('the', 6283), ('and', 5680), ('to', 4766), ('i', 4653), ('of', 3757)]\n",
      "Index Map Examples:\n",
      " {0: '<unk>', 1: 'the', 2: 'and', 3: 'to', 4: 'i'}\n",
      "\n",
      "Examples of CBOW batches:\n",
      "Batch shape: (8, 4)\n",
      "Labels shape: (8, 1)\n",
      "Context words: ['first', 'citizen', 'we', 'proceed'], Target word: before\n",
      "Context words: ['citizen', 'before', 'proceed', 'any'], Target word: we\n",
      "Context words: ['before', 'we', 'any', 'further'], Target word: proceed\n",
      "Context words: ['we', 'proceed', 'further', 'hear'], Target word: any\n",
      "Context words: ['proceed', 'any', 'hear', 'me'], Target word: further\n",
      "\n",
      "Examples of Skip-gram batches:\n",
      "Batch shape: (8,)\n",
      "Labels shape: (8, 1)\n",
      "Input word: ['before', 'before', 'before', 'before', 'we', 'we', 'we', 'we'], Output word: proceed\n",
      "Input word: ['before', 'before', 'before', 'before', 'we', 'we', 'we', 'we'], Output word: we\n",
      "Input word: ['before', 'before', 'before', 'before', 'we', 'we', 'we', 'we'], Output word: first\n",
      "Input word: ['before', 'before', 'before', 'before', 'we', 'we', 'we', 'we'], Output word: citizen\n",
      "Input word: ['before', 'before', 'before', 'before', 'we', 'we', 'we', 'we'], Output word: before\n"
     ]
    }
   ],
   "source": [
    "## CBOW is functioning / Skipgram is not functioning -- Code below can be made into a debug function\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = prepareData(words)\n",
    "\n",
    "# Call the cbow function\n",
    "batch_size = 8\n",
    "num_skips = 4\n",
    "skip_window = 2\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Corpus length: {len(data)}\")\n",
    "print(f\"Encoded corpus sample: {data[:20]}\")\n",
    "print(f\"Decoded corpus sample: {[reverse_dictionary[i] for i in data[:20]]}\")\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(count)}\")\n",
    "print(f\"Index Map size: {len(dictionary)}\")\n",
    "print(f\"\\nMost common words:\\n{sorted(count, key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "print(\"Index Map Examples:\\n\", {k: reverse_dictionary[k] for k in list(reverse_dictionary)[:5]})\n",
    "\n",
    "# Print the first 5 examples of CBOW batches\n",
    "print(\"\\nExamples of CBOW batches:\")\n",
    "cbow_batch, cbow_labels = cbow(data, batch_size, num_skips, skip_window)\n",
    "print(f\"Batch shape: {cbow_batch.shape}\")\n",
    "print(f\"Labels shape: {cbow_labels.shape}\")\n",
    "for i in range(5):\n",
    "    context_words = [reverse_dictionary[idx] for idx in cbow_batch[i]]\n",
    "    target_word = reverse_dictionary[cbow_labels[i, 0]]\n",
    "    print(f\"Context words: {context_words}, Target word: {target_word}\")\n",
    "\n",
    "# Print the first 5 examples of Skip-gram batches\n",
    "print(\"\\nExamples of Skip-gram batches:\")\n",
    "skipgram_batch, skipgram_labels = skipgram(data, batch_size, num_skips, skip_window)\n",
    "print(f\"Batch shape: {skipgram_batch.shape}\")\n",
    "print(f\"Labels shape: {skipgram_labels.shape}\")\n",
    "for i in range(5):\n",
    "    input_words = [reverse_dictionary[idx] for idx in skipgram_batch]\n",
    "    output_word = reverse_dictionary[skipgram_labels[i, 0]]\n",
    "    print(f\"Input word: {input_words}, Output word: {output_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocab_size: int = 50000,\n",
    "                 batch_size: int = 128,\n",
    "                 embedding_size: int = 128,\n",
    "                 architecture: str = 'skip-gram',\n",
    "                 num_skips: int = 2,\n",
    "                 skip_window: int = 1,\n",
    "                 loss_type: str = 'sampled_softmax_loss',\n",
    "                 n_neg_samples: int = 64,\n",
    "                 optimizer: str = 'adagrad',\n",
    "                 learning_rate: float = 1.0,\n",
    "                 n_steps: int = 10001,\n",
    "                 valid_size: int = 16,\n",
    "                 valid_window: int = 100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.architecture = architecture\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        self.loss_type = loss_type\n",
    "        self.n_neg_samples = n_neg_samples\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_steps = n_steps\n",
    "        self.valid_size = valid_size\n",
    "        self.valid_window = valid_window\n",
    "\n",
    "        self.chooseSamples()\n",
    "        self.chooseGenerator()\n",
    "        self.__init__model()\n",
    "\n",
    "    def chooseSamples(self):\n",
    "        valid_examples = np.array(random.sample(range(self.valid_window), self.valid_size))\n",
    "        self.valid_examples = valid_examples\n",
    "    \n",
    "    def chooseGenerator(self):\n",
    "        if self.architecture == 'skip-gram':\n",
    "            self.generator = skipgram\n",
    "        elif self.architecture == 'cbow':\n",
    "            self.generator = cbow\n",
    "        else:\n",
    "            raise ValueError(\"Architecture must be either 'skip-gram' or 'cbow'.\")\n",
    "        \n",
    "    def tokenMapping(self, words):\n",
    "        data, count, dictionary, reverse_dictionary = prepareData(words, self.vocab_size)\n",
    "        self.data = data\n",
    "        self.count = count\n",
    "        self.dictionary = dictionary\n",
    "        self.reverse_dictionary = reverse_dictionary\n",
    "        return data\n",
    "    \n",
    "    def __init__model(self):\n",
    "        self.embeddings = tf.Variable(tf.random.uniform([self.vocab_size, self.embedding_size], -1.0, 1.0))\n",
    "        self.weights = tf.Variable(tf.random.truncated_normal([self.vocab_size, self.embedding_size], stddev=1.0 / np.sqrt(self.embedding_size)))\n",
    "        self.biases = tf.Variable(tf.zeros([self.vocab_size]))\n",
    "\n",
    "        if self.optimizer == 'adagrad':\n",
    "            self.optimizer = tf.optimizers.Adagrad(learning_rate=self.learning_rate)\n",
    "        elif self.optimizer == 'SGD':\n",
    "            self.optimizer = tf.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Compute the similarity distance metrics between individual embeddings\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(self.embeddings), 1, keepdims=True))\n",
    "        self.normalized_embeddings = self.embeddings / norm\n",
    "        self.valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)\n",
    "        self.valid_embeddings = tf.nn.embedding_lookup(self.normalized_embeddings, self.valid_dataset)\n",
    "        self.similarity = tf.matmul(self.valid_embeddings, self.normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, batch_data, batch_labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            if self.architecture == 'skip-gram':\n",
    "                embed = tf.nn.embedding_lookup(self.embeddings, batch_data)\n",
    "            elif self.architecture == 'cbow':\n",
    "                embed = tf.zeros([self.batch_size, self.embedding_size])\n",
    "                for j in range(self.num_skips):\n",
    "                    embed += tf.nn.embedding_lookup(self.embeddings, batch_data[:, j])\n",
    "                embed /= self.num_skips\n",
    "        \n",
    "            if self.loss_type == 'sampled_softmax_loss':\n",
    "                loss = tf.nn.sampled_softmax_loss(weights=self.weights,\n",
    "                                                  biases=self.biases,\n",
    "                                                  labels=batch_labels,\n",
    "                                                  inputs=embed,\n",
    "                                                  num_sampled=self.n_neg_samples,\n",
    "                                                  num_classes=self.vocab_size)\n",
    "            elif self.loss_type == 'nce_loss':\n",
    "                loss = tf.nn.nce_loss(weights=self.weights,\n",
    "                                      biases=self.biases,\n",
    "                                      labels=batch_labels,\n",
    "                                      inputs=embed,\n",
    "                                      num_sampled=self.n_neg_samples,\n",
    "                                      num_classes=self.vocab_size)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, [self.embeddings, self.weights, self.biases])\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.embeddings, self.weights, self.biases]))\n",
    "        return loss\n",
    "\n",
    "    def fit(self, words):\n",
    "        self.data = self.tokenMapping(words)\n",
    "        average_loss = 0\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            batch_data, batch_labels = self.generator(self.data, self.batch_size, self.num_skips, self.skip_window)\n",
    "            loss = self.train_step(batch_data, batch_labels)\n",
    "            average_loss += loss\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                print(f'Average loss at step {step}: {average_loss}')\n",
    "                average_loss = 0\n",
    "\n",
    "        self.final_embeddings = self.normalized_embeddings.numpy()\n",
    "        return self\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        if word not in self.dictionary:\n",
    "            raise ValueError(f\"Word '{word}' not in dictionary\")\n",
    "        word_index = self.dictionary[word]\n",
    "        return self.final_embeddings[word_index]\n",
    "\n",
    "    def similar_by_word(self, word, top_n=5):\n",
    "        word_vector = self.get_embedding(word)\n",
    "        similarities = np.dot(self.final_embeddings, word_vector) / (np.linalg.norm(self.final_embeddings, axis=1) * np.linalg.norm(word_vector))\n",
    "        similar_indices = np.argsort(-similarities)[:top_n]\n",
    "        \n",
    "        print(f\"Similar indices for '{word}': {similar_indices}\")\n",
    "        similar_words = []\n",
    "        for idx in similar_indices:\n",
    "            if idx in self.reverse_dictionary:\n",
    "                similar_words.append(self.reverse_dictionary[idx])\n",
    "            else:\n",
    "                print(f\"Index {idx} not found in reverse_dictionary\")\n",
    "                similar_words.append(f\"Index {idx} not found\")\n",
    "        \n",
    "        return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 8.060439109802246\n",
      "Average loss at step 2000: 0.048118025064468384\n",
      "Average loss at step 4000: 0.0013527171686291695\n",
      "Average loss at step 6000: 0.0008192718378268182\n",
      "Average loss at step 8000: 0.0005979723064228892\n",
      "Average loss at step 10000: 0.00046493535046465695\n"
     ]
    }
   ],
   "source": [
    "# Call the cbow function\n",
    "batch_size = 128\n",
    "num_skips = 4\n",
    "skip_window = 2\n",
    "\n",
    "word2vec = Word2Vec(architecture='cbow', batch_size=batch_size, num_skips=num_skips, skip_window=skip_window)\n",
    "model = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'king': [ 0.097679    0.11546155  0.04706017  0.0797425  -0.0867006   0.10976204\n",
      " -0.02722485  0.11806993 -0.09394931  0.14937137  0.15121503 -0.13773635\n",
      " -0.05374016 -0.03249441 -0.13612342 -0.00243258  0.08075307  0.05630411\n",
      " -0.09662239  0.12566575 -0.05936121 -0.01748659 -0.15297791  0.02653886\n",
      "  0.08502681  0.00281646 -0.119552    0.07002008 -0.0173407  -0.05168995\n",
      " -0.08079509 -0.0576941  -0.028062   -0.08680786 -0.02232949 -0.0872113\n",
      " -0.04099102 -0.10084173 -0.0040462  -0.10600445  0.09158263 -0.09803469\n",
      " -0.0528405  -0.04636468  0.10959921  0.06783292  0.12982303 -0.02782906\n",
      "  0.06420797 -0.00268754 -0.09327883  0.05187593  0.00748487  0.02019454\n",
      "  0.14745869 -0.03426331  0.04124076 -0.10454046  0.04547433 -0.10629994\n",
      "  0.13306089 -0.09884022 -0.02279956  0.12004954  0.06989671  0.14381188\n",
      "  0.01204047  0.1449261   0.07300401  0.07294932  0.12616472 -0.12199733\n",
      "  0.14380695  0.03529338  0.12421209 -0.13357255 -0.04246579  0.11562449\n",
      " -0.12807144 -0.04317658  0.09381955  0.11074334 -0.10791184  0.04460761\n",
      "  0.07530826  0.11257459 -0.00507503 -0.05339769 -0.12005264  0.03571109\n",
      "  0.12327556  0.14712502  0.09079477  0.00655038 -0.10084885  0.0428749\n",
      "  0.0027898   0.0670926   0.09488201  0.08510228  0.13576832 -0.08383898\n",
      "  0.09522408 -0.01442836  0.01543058 -0.09894479 -0.08317098 -0.10825267\n",
      "  0.08179906 -0.12616518  0.08881933 -0.00272449  0.05103261 -0.03524752\n",
      " -0.09045321 -0.04925693 -0.03300684 -0.05062494  0.09863628  0.05035078\n",
      "  0.1023292  -0.02666117 -0.15145421  0.05541822  0.08647635  0.11195301\n",
      "  0.15154298  0.04542566]\n",
      "Similar indices for 'king': [   34  1147 10578 16616 12906]\n",
      "Index 16616 not found in reverse_dictionary\n",
      "Index 12906 not found in reverse_dictionary\n",
      "Words similar to 'king': ['king', 'corioli', 'nostril', 'Index 16616 not found', 'Index 12906 not found']\n"
     ]
    }
   ],
   "source": [
    "# Find the embedding for a word\n",
    "try:\n",
    "    embedding = model.get_embedding('king')\n",
    "    print(f\"Embedding for 'king': {embedding}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Find similar words\n",
    "try:\n",
    "    similar_words = model.similar_by_word('king')\n",
    "    print(f\"Words similar to 'king': {similar_words}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
