{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from word_embeddings import Word2Vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "- Fix Skipgram\n",
    "- Fix out of vocab_size index issues\n",
    "- Build test method/function for use when imported\n",
    "- Test word2vec embeddings with wiki dataset\n",
    "- Fix fit method to match fit_from_tokens\n",
    "- Clean wiki dataset by removing extra sections at the end (ie. see also, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 202651\n",
      "\n",
      "Tokenizing words...\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|           | Elapsed: 00:01 | ETA: 07:59 | 20.80it/s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 6.7142415046691895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  20%|██         | Elapsed: 00:09 | ETA: 00:36 | 218.89it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2000: 0.0020651656668633223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  40%|████       | Elapsed: 00:18 | ETA: 00:24 | 246.07it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4000: 0.00010465641389600933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  60%|██████     | Elapsed: 00:26 | ETA: 00:18 | 216.10it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 6000: 6.053332253941335e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  80%|████████   | Elapsed: 00:34 | ETA: 00:09 | 208.95it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 8000: 4.3287454900564626e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████ | Elapsed: 00:43 | ETA: 00:00 | 227.96it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10000: 3.232683957321569e-05\n",
      "\n",
      "Training has completed successfully.\n",
      "\n",
      "Similar indices for 'king': [   34  7989   442 10166  5892]\n",
      "\n",
      "Embedding for 'king': [-0.03580251 -0.02700209  0.06671405  0.01987702 -0.0750451   0.03752913\n",
      "  0.09966585  0.08348835 -0.08564416 -0.02801651  0.07077959 -0.01931835\n",
      " -0.11053766  0.03350592  0.07308815  0.07094621  0.14404781  0.00347055\n",
      " -0.01321465  0.11222426 -0.01781661 -0.11343024 -0.09006338 -0.1465569\n",
      "  0.08967006 -0.13756578  0.07960131 -0.02542244 -0.12340839  0.0739371\n",
      " -0.00245022 -0.0705544  -0.14847781 -0.02540014 -0.0341523  -0.11598612\n",
      "  0.14166783 -0.11128858  0.00839817 -0.03771213  0.06894025  0.06559727\n",
      "  0.09567337  0.08064143 -0.11779281  0.0493288   0.12730995  0.1357397\n",
      "  0.1379387  -0.14997934 -0.00221588  0.0552395   0.03114419  0.02987166\n",
      " -0.00979318 -0.0303425  -0.08213712  0.04316232 -0.11107747 -0.01184592\n",
      "  0.09652275 -0.10239444  0.03358982 -0.07328041  0.09003548  0.13210207\n",
      "  0.06599475 -0.1328156  -0.02956748 -0.1382575  -0.08305071  0.13498063\n",
      "  0.12832619 -0.09133114 -0.13768959 -0.04945831 -0.14162637  0.12354962\n",
      " -0.06986514 -0.02038973 -0.14314203  0.04879791  0.00351946 -0.09183474\n",
      " -0.04528946 -0.12823333 -0.10361846  0.06077484  0.07652895  0.11228715\n",
      "  0.05420515  0.12168511 -0.09084388 -0.13221674  0.12783244  0.02785065\n",
      "  0.05541099 -0.06136661 -0.11102487  0.0735956   0.0902932   0.08937219\n",
      "  0.05492065 -0.11145846  0.02269772 -0.01501505 -0.0913339   0.09163994\n",
      "  0.02961029 -0.1210516  -0.08520532  0.07303495 -0.10517506  0.06692853\n",
      "  0.08710536 -0.02395867  0.0205201   0.06199262 -0.12682395 -0.11806472\n",
      "  0.13838874 -0.12173498  0.04755253 -0.11907365  0.05805801  0.07615574\n",
      " -0.10585722  0.03092294]\n",
      "\n",
      "Words similar to 'king': ['king', 'outgrown', 'foul', 'misdeed', 'knots']\n",
      "\n",
      "The word closest to 'king - man + woman' is: woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "with open(path_to_file) as f:\n",
    "    words = [word for line in f.readlines() for word in line.split()]\n",
    "\n",
    "print(f'Number of words: {len(words)}')\n",
    "\n",
    "new_model = Word2Vec(vocab_size=12000,\n",
    "                     batch_size=8,\n",
    "                     num_skips=4, \n",
    "                     skip_window=2,\n",
    "                     architecture='cbow')\n",
    "new_model.fit(words)\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in shakespeare: 25670\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(words)\n",
    "print(f\"Unique words in shakespeare: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words in shakespeare: 12848\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "vocab_size = 50000\n",
    "\n",
    "## Corpus pre-processing\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "words = [word.lower().translate(translator) for word in words]\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "## Get word counts for vocabulary with <unk> token to replace rare words\n",
    "count = [['<unk>', -1]]\n",
    "count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "print(f\"Counted words in shakespeare: {len(count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram (data: List[int], batch_size: int, num_skips: int, skip_window: int, data_index: int = 0):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for the skip-gram model.\n",
    "\n",
    "    Parameters:\n",
    "    data:        List of word indices.\n",
    "    batch_size:  Number of words in each batch.\n",
    "    num_skips:   How many times to reuse an input to generate a label.\n",
    "    skip_window: How many words to consider left and right.\n",
    "    data_index:  Index to start with in the data list. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Batch of input words and corresponding labels.\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data)\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    window_size = 2 * skip_window + 1\n",
    "\n",
    "    # Create a buffer to store the data\n",
    "    buffer = deque(maxlen=window_size)\n",
    "    for _ in range(window_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    # Generates the batch of context words and labels\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, window_size - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "        # Move the window\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CBOW is functioning / Skipgram is not functioning -- Code below can be made into a debug function\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = prepareData(words)\n",
    "\n",
    "# Call the cbow function\n",
    "batch_size = 8\n",
    "num_skips = 4\n",
    "skip_window = 2\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Corpus length: {len(data)}\")\n",
    "print(f\"Encoded corpus sample: {data[:20]}\")\n",
    "print(f\"Decoded corpus sample: {[reverse_dictionary[i] for i in data[:20]]}\")\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(count)}\")\n",
    "print(f\"Index Map size: {len(dictionary)}\")\n",
    "print(f\"\\nMost common words:\\n{sorted(count, key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "print(\"Index Map Examples:\\n\", {k: reverse_dictionary[k] for k in list(reverse_dictionary)[:5]})\n",
    "\n",
    "# Print the first 5 examples of CBOW batches\n",
    "print(\"\\nExamples of CBOW batches:\")\n",
    "cbow_batch, cbow_labels = cbow(data, batch_size, num_skips, skip_window)\n",
    "print(f\"Batch shape: {cbow_batch.shape}\")\n",
    "print(f\"Labels shape: {cbow_labels.shape}\")\n",
    "for i in range(5):\n",
    "    context_words = [reverse_dictionary[idx] for idx in cbow_batch[i]]\n",
    "    target_word = reverse_dictionary[cbow_labels[i, 0]]\n",
    "    print(f\"Context words: {context_words}, Target word: {target_word}\")\n",
    "\n",
    "# Print the first 5 examples of Skip-gram batches\n",
    "print(\"\\nExamples of Skip-gram batches:\")\n",
    "skipgram_batch, skipgram_labels = skipgram(data, batch_size, num_skips, skip_window)\n",
    "print(f\"Batch shape: {skipgram_batch.shape}\")\n",
    "print(f\"Labels shape: {skipgram_labels.shape}\")\n",
    "for i in range(5):\n",
    "    input_words = [reverse_dictionary[idx] for idx in skipgram_batch]\n",
    "    output_word = reverse_dictionary[skipgram_labels[i, 0]]\n",
    "    print(f\"Input word: {input_words}, Output word: {output_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = 'Resources\\\\corpus.pkl'\n",
    "batch_size = 128\n",
    "num_skips = 4\n",
    "skip_window = 2\n",
    "\n",
    "word2vec = Word2Vec(architecture='cbow', batch_size=batch_size, num_skips=num_skips, skip_window=skip_window)\n",
    "model = word2vec.fit(pickle_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = Word2Vec(architecture='cbow', batch_size=1000, num_skips=4, skip_window=2, vocab_size=500000, n_steps=1)\n",
    "test_model.fit_from_tokens(test_data, test_count, test_dictionary, test_reverse_dictionary)\n",
    "test_model.eval()\n",
    "\n",
    "# Find the embedding for a word\n",
    "try:\n",
    "    embedding_king = test_model.get_embedding('king')\n",
    "    print(f\"Embedding for 'king': {embedding}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Find similar words\n",
    "try:\n",
    "    similar_words = test_model.similar_by_word('king')\n",
    "    print(f\"Words similar to 'king': {similar_words}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Find the embeddings for the words\n",
    "try:\n",
    "    embedding_man = test_model.get_embedding('man')\n",
    "    embedding_woman = test_model.get_embedding('woman')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Perform the vector arithmetic: king\n",
    "result_vector = embedding_king - embedding_man + embedding_woman\n",
    "\n",
    "# Find the word closest to the resulting vector\n",
    "try:\n",
    "    closest_word = test_model.similar_by_vector(result_vector, topn=1)[0][0]\n",
    "    print(f\"The word closest to result is: {closest_word}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
